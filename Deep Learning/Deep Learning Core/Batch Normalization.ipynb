{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8da97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Theory and Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00f293",
   "metadata": {},
   "source": [
    "Concept of Batch Normalization: \n",
    "    \n",
    "    Batch normalization is a technique used in artificial neural networks to improve the stability and speed of training. It normalizes the input of each layer by adjusting and scaling the activations. This normalization is performed over mini-batches of data during training.\n",
    "\n",
    "\n",
    "Benefits of Batch Normalization:\n",
    "\n",
    "Improved Training Stability: \n",
    "    \n",
    "    Batch normalization reduces the internal covariate shift, making training more stable and allowing for the use of higher learning rates.\n",
    "Faster Convergence: \n",
    "    \n",
    "    By reducing internal covariate shift and ensuring that inputs are within a similar range, batch normalization accelerates the convergence of the training process.\n",
    "Regularization Effect:\n",
    "    \n",
    "    Batch normalization includes a slight regularization effect, which can reduce the need for dropout or other regularization techniques.\n",
    "Mitigation of Vanishing/Exploding Gradient Problem: \n",
    "    \n",
    "    Batch normalization helps alleviate the vanishing or exploding gradient problem by ensuring that gradients propagated through the network are within reasonable ranges.\n",
    "\n",
    "\n",
    "\n",
    "Working Principle of Batch Normalization:\n",
    "\n",
    "Normalization Step:\n",
    "    \n",
    "    In batch normalization, the input to each layer is normalized to have zero mean and unit variance. This step is performed independently for each feature in each mini-batch.\n",
    "Learnable Parameters: \n",
    "    \n",
    "    Batch normalization introduces learnable parameters, typically scaling and shifting parameters, for each normalized feature. These parameters allow the network to learn the optimal scale and shift for each feature.\n",
    "During Training: \n",
    "    \n",
    "    During training, batch normalization calculates the mean and variance of each feature across the mini-batch. It then normalizes the features using these statistics and scales and shifts them using the learnable parameters.\n",
    "During Inference:\n",
    "    \n",
    "    During inference, batch normalization uses the estimated population statistics (mean and variance) calculated during training to normalize the input. This ensures consistent behavior between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ed0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Step 1: Preprocess the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Step 2: Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Train the neural network without batch normalization\n",
    "def train(model, criterion, optimizer, train_loader, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Instantiate the model, criterion, and optimizer\n",
    "model_without_bn = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_without_bn.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model without batch normalization\n",
    "train(model_without_bn, criterion, optimizer, train_loader)\n",
    "\n",
    "# Step 4: Implement batch normalization layers\n",
    "class SimpleNNWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNWithBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Step 5: Train the model with batch normalization\n",
    "model_with_bn = SimpleNNWithBN()\n",
    "optimizer_bn = optim.SGD(model_with_bn.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model with batch normalization\n",
    "train(model_with_bn, criterion, optimizer_bn, train_loader)\n",
    "\n",
    "# Step 6: Compare performance\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Performance without Batch Normalization:\")\n",
    "test(model_without_bn, test_loader)\n",
    "\n",
    "print(\"Performance with Batch Normalization:\")\n",
    "test(model_with_bn, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1232f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. Experimentation and Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def42de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experimentation and Analysis:\n",
    "\n",
    "Experimenting with Different Batch Sizes:\n",
    "\n",
    "Effect on Training Dynamics: Varying the batch size can have a significant impact on the training dynamics. Smaller batch sizes may lead to faster convergence as each update is computed more frequently, but they can also result in more noisy gradients and slower convergence in some cases. Larger batch sizes can provide smoother gradients but may require more memory and computational resources.\n",
    "Effect on Model Performance: The choice of batch size can affect the final performance of the model. Smaller batch sizes might generalize better as they introduce more stochasticity during training, potentially helping the model escape local minima. However, larger batch sizes might lead to faster convergence and better utilization of hardware resources.\n",
    "Advantages and Potential Limitations of Batch Normalization:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Stabilized Training: Batch normalization helps stabilize the training process by reducing internal covariate shift. This leads to faster convergence and allows the use of higher learning rates.\n",
    "Regularization: Batch normalization acts as a form of regularization, reducing the need for other regularization techniques such as dropout.\n",
    "Improved Gradient Flow: By normalizing the input to each layer, batch normalization mitigates the vanishing and exploding gradient problems, making it easier to train deeper networks.\n",
    "Improved Generalization: Batch normalization introduces noise during training, which can act as a form of implicit regularization, helping the model generalize better to unseen data.\n",
    "Potential Limitations:\n",
    "\n",
    "Increased Computational Overhead: Batch normalization adds computational overhead during both training and inference, as it requires additional calculations for normalization and scaling.\n",
    "Sensitivity to Batch Size: Batch normalization's performance can be sensitive to the choice of batch size. Very small batch sizes might lead to inaccurate estimates of batch statistics, while very large batch sizes might reduce the effectiveness of the normalization.\n",
    "Dependency on Mini-batch Statistics: During inference, batch normalization relies on estimated batch statistics computed during training. If the distribution of the input data during inference differs significantly from that during training, performance may be affected.\n",
    "Limited Applicability to Some Architectures: While batch normalization is widely used in feedforward and convolutional neural networks, its applicability to recurrent neural networks (RNNs) and other architectures may be limited due to the sequential nature of data processing in these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a7fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb079651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca6f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
