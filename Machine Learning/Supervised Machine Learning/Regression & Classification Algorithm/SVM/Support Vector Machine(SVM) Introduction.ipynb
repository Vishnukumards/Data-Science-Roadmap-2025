{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1baea3d5-4a04-4229-abe3-4852b32d6437",
   "metadata": {},
   "source": [
    "\r\n",
    "---\r\n",
    "\r\n",
    "## üåü Support Vector Machines (SVM) ‚Äî Simple Summary\r\n",
    "\r\n",
    "### üìå What is SVM?\r\n",
    "\r\n",
    "SVM is a machine learning method that draws the **best boundary** between two groups in data. It tries to **keep this boundary as far away as possible** from the closest points on both sides.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üîç Core Concepts\r\n",
    "\r\n",
    "* **Hyperplane** = A line/plane that separates classes.\r\n",
    "* **Margin** = Distance between the hyperplane and the nearest points.\r\n",
    "* **Support Vectors** = Data points **closest to the margin** ‚Äî they define the boundary.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ü§ñ SVC (Support Vector Classifier)\r\n",
    "\r\n",
    "* **Used for:** Classification (e.g., spam vs. not spam).\r\n",
    "* **Goal:** Find the widest possible margin between classes.\r\n",
    "\r\n",
    "#### Hard Margin:\r\n",
    "\r\n",
    "* No mistakes allowed.\r\n",
    "* **Only works** when data is perfectly separable.\r\n",
    "* Very sensitive to outliers.\r\n",
    "\r\n",
    "#### Soft Margin:\r\n",
    "\r\n",
    "* Allows **some mistakes**.\r\n",
    "* Works **better on real-world data**.\r\n",
    "* Controlled by **C**:\r\n",
    "\r\n",
    "  * **High C** = less errors, more strict.\r\n",
    "  * **Low C** = more flexible, better generalization.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üåÄ Kernels (For non-linear data)\r\n",
    "\r\n",
    "* Kernels let SVM work with **curved boundaries**.\r\n",
    "* They \"trick\" the model into thinking the data is in a higher dimension.\r\n",
    "\r\n",
    "#### Common Kernels:\r\n",
    "\r\n",
    "| Type       | Use when...                   |\r\n",
    "| ---------- | ----------------------------- |\r\n",
    "| Linear     | Data is already separable     |\r\n",
    "| RBF        | Complex, unknown patterns     |\r\n",
    "| Polynomial | Data has curved relationships |\r\n",
    "| Sigmoid    | Rarely used, like neural nets |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üîß Important Parameters (SVC)\r\n",
    "\r\n",
    "* **C** = Controls margin flexibility.\r\n",
    "* **Kernel** = Type of transformation.\r\n",
    "* **Gamma** = Controls how far a single point can affect the boundary.\r\n",
    "* **Degree** = Used in polynomial kernels.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üìè SVM for Regression (SVR)\r\n",
    "\r\n",
    "* **Used for:** Predicting numbers (not classes).\r\n",
    "* Fits a line or curve within a margin (called **epsilon-tube**).\r\n",
    "* **Epsilon** = How much error we allow without penalty.\r\n",
    "* **C** = Balances smoothness vs. closeness to the data.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ‚úÖ Pros of SVM\r\n",
    "\r\n",
    "* Works well with **small** and **high-dimensional** data.\r\n",
    "* Powerful with the **right kernel**.\r\n",
    "* Focuses only on important data points (support vectors).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ‚ùå Cons of SVM\r\n",
    "\r\n",
    "* Can be **slow** with very large data.\r\n",
    "* Needs **feature scaling**.\r\n",
    "* Not grrn this into a **1-page cheat sheet** or give examples for each section ‚Äî just let me know!\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478209ef-2c01-46b4-8f2b-9774863c439f",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e98e82-6193-44da-b37c-c135450656c3",
   "metadata": {},
   "source": [
    "g them.\"\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ‚úÖ **SVC (Support Vector **Classifier**):**\r\n",
    "\r\n",
    "Used when the goal is to **classify things** (like cat vs. dog, spam vs. not-spam).\r\n",
    "\r\n",
    "* It draws a line (or curve) that **separates classes** with the **maximum gap** between them.\r\n",
    "* If perfect separation isn‚Äôt possible, it allows some mistakes but still tries to keep the gap wide (this is the **soft margin** idea).\r\n",
    "\r\n",
    "**Easy Example:**\r\n",
    "\"Is this email spam or not?\" ‚Äî SVC finds the best rule (line) to decide that.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üìà **SVR (Support Vector **Regressor**):**\r\n",
    "\r\n",
    "Used when the goal is to **predict numbers** (like house prices, temperatures).\r\n",
    "\r\n",
    "* It tries to draw a **line that fits the data**, but allows a small **tolerance (Œµ)** where errors are okay (called the Œµ-tube).\r\n",
    "* Only points **outside** this tube affect the model.\r\n",
    "\r\n",
    "**Easy Example:**\r\n",
    "\"What will be the house price next year?\" ‚Äî SVR draws a line that predicts it, ignoring small errors.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üîë Simple Summary:\r\n",
    "\r\n",
    "| Method  | What It Does                                              | Use Case                          |\r\n",
    "| ------- | --------------------------------------------------------- | --------------------------------- |\r\n",
    "| **SVM** | A base method that finds the best boundary or fit         | Classification or regression      |\r\n",
    "| **SVC** | Classifies things by separating them with a wide margin   | Spam detection, image recognition |\r\n",
    "| **SVR** | Predicts numbers by fitting a line within an error margin | Price prediction, time series     |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Let me know if you want simple visuals or analogies too!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54f94f-8358-43a0-9bd9-554b70b488fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a6c7067-50cf-435f-bec2-9255c891b5bc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Support Vector Machine (SVM) ‚Äì Math Intuition\n",
    "\n",
    "SVM is a **supervised learning algorithm** used for **classification** (SVC) and **regression** (SVR). The core idea is to **find the optimal hyperplane** that **best separates the data** in high-dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. SVC (Support Vector Classification)\n",
    "\n",
    "### ‚úÖ Goal:\n",
    "\n",
    "Find a hyperplane that **maximally separates classes**.\n",
    "\n",
    "### üßÆ Core Math:\n",
    "\n",
    "Assume binary classification: labels are $y_i \\in \\{-1, +1\\}$, input features $\\mathbf{x}_i \\in \\mathbb{R}^n$\n",
    "\n",
    "The **decision function** (a hyperplane):\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "We want to:\n",
    "\n",
    "* **Maximize margin**: distance between the hyperplane and the nearest data points\n",
    "* These nearest points are **support vectors**\n",
    "\n",
    "### üéØ Optimization Objective:\n",
    "\n",
    "Minimize:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "$$\n",
    "\n",
    "Subject to constraints (for hard margin SVM):\n",
    "\n",
    "$$\n",
    "y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1\n",
    "$$\n",
    "\n",
    "This is a **quadratic optimization problem with linear constraints**.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Soft Margin SVM (real-world use)\n",
    "\n",
    "Real data isn't perfectly separable. So we introduce slack variables $\\xi_i$:\n",
    "\n",
    "$$\n",
    "y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "\n",
    "Minimize:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
    "$$\n",
    "\n",
    "* $C$: regularization parameter (trade-off between margin size and misclassification)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Kernel Trick (Nonlinear SVM):\n",
    "\n",
    "When data is not linearly separable in input space, **kernel functions** project it into higher-dimensional space:\n",
    "\n",
    "Common kernels:\n",
    "\n",
    "* Linear: $K(x, x') = x^\\top x'$\n",
    "* RBF (Gaussian): $K(x, x') = \\exp\\left(-\\gamma \\|x - x'\\|^2\\right)$\n",
    "* Polynomial: $K(x, x') = (x^\\top x' + c)^d$\n",
    "\n",
    "Optimization is done in terms of **dual form**, using Lagrange multipliers $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_i \\alpha_i y_i K(x_i, x) + b\n",
    "$$\n",
    "\n",
    "Only **support vectors** have $\\alpha_i > 0$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è Simple SVC Example\n",
    "\n",
    "Suppose we have 2D points:\n",
    "\n",
    "| x‚ÇÅ | x‚ÇÇ | y  |\n",
    "| -- | -- | -- |\n",
    "| 1  | 2  | 1  |\n",
    "| 2  | 3  | 1  |\n",
    "| 2  | 0  | -1 |\n",
    "| 3  | 1  | -1 |\n",
    "\n",
    "SVM will find a line (in 2D) that separates class +1 and -1 with maximum margin.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∏ 2. SVR (Support Vector Regression)\n",
    "\n",
    "### ‚úÖ Goal:\n",
    "\n",
    "Fit a function that approximates target values **within an epsilon-tube** (a margin of tolerance).\n",
    "\n",
    "### üßÆ Core Math:\n",
    "\n",
    "Given training data $(\\mathbf{x}_i, y_i)$, SVR tries to fit a function:\n",
    "\n",
    "$$\n",
    "f(x) = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Minimize:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b &\\leq \\epsilon + \\xi_i \\\\\n",
    "\\mathbf{w}^\\top \\mathbf{x}_i + b - y_i &\\leq \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* &\\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* $\\epsilon$: epsilon-tube width (allowed deviation from true y)\n",
    "* $\\xi_i, \\xi_i^*$: slack variables for exceeding epsilon\n",
    "* $C$: penalty for exceeding epsilon\n",
    "\n",
    "Only points **outside the epsilon-tube** become **support vectors**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è Simple SVR Example\n",
    "\n",
    "Let‚Äôs say your data points lie around a straight line:\n",
    "\n",
    "* Input: $x = [1, 2, 3, 4]$\n",
    "* Output: $y = [2.1, 3.9, 6.2, 7.8]$\n",
    "\n",
    "SVR with $\\epsilon = 0.5$ will try to fit a line that passes within a ¬±0.5 margin around most data points.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Geometric Intuition\n",
    "\n",
    "* **SVC**: Separates data into distinct classes with maximum margin\n",
    "* **SVR**: Tries to keep predictions close to true values within a margin\n",
    "\n",
    "In both:\n",
    "\n",
    "* Only a subset of points **(support vectors)** matter in the final model\n",
    "* Optimization happens in **high-dimensional space**, often using **kernel trick**\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Summary Table\n",
    "\n",
    "| Feature     | SVC                                     | SVR                                                |\n",
    "| ----------- | --------------------------------------- | -------------------------------------------------- |\n",
    "| Task        | Classification                          | Regression                                         |\n",
    "| Output      | Class label                             | Continuous value                                   |\n",
    "| Objective   | Maximize margin                         | Fit within Œµ-tube                                  |\n",
    "| Slack       | Misclassification (Œæ)                   | Error above Œµ (Œæ, Œæ\\*)                             |\n",
    "| Key Param   | C, kernel, Œ≥                            | C, Œµ, kernel, Œ≥                                    |\n",
    "| Output Func | $f(x) = \\sum \\alpha_i y_i K(x_i,x) + b$ | $f(x) = \\sum (\\alpha_i - \\alpha_i^*) K(x_i,x) + b$ |\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like, I can show a visualization of how the margin, support vectors, and hyperplanes work in 2D using example data. Just say the word!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518b0bc-07b0-4a32-a294-9a34a0f12098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8dcd9b6-0472-4569-8eb7-c0b45a93a023",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üåü Core Idea (Applies to All: SVM / SVC / SVR)\n",
    "\n",
    "> **SVM tries to find the \"best hyperplane\"** to either **separate data (SVC)** or **fit data (SVR)** with the **maximum margin** and **minimum error**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **SVC (Support Vector Classifier)**\n",
    "\n",
    "### üîß Concepts:\n",
    "\n",
    "* **Hyperplane**: A line/plane that separates classes.\n",
    "* **Margin**: Distance from hyperplane to nearest points (support vectors).\n",
    "* **Support Vectors**: Critical points that define the margin.\n",
    "* **Kernel Trick**: Maps data to higher dimensions if not linearly separable.\n",
    "* **Soft Margin**: Allows some misclassifications (C parameter controls this).\n",
    "\n",
    "### üéØ Objective:\n",
    "\n",
    "* Maximize margin\n",
    "* Minimize misclassification errors\n",
    "\n",
    "### üßæ Output:\n",
    "\n",
    "* Predicted **class labels** (e.g., `0` or `1`, or `cat` or `dog`)\n",
    "* Optional: Class **probabilities** (if enabled)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **SVR (Support Vector Regressor)**\n",
    "\n",
    "### üîß Concepts:\n",
    "\n",
    "* **Epsilon Tube (Œµ)**: A margin of tolerance ‚Äî no penalty for predictions inside this tube.\n",
    "* **Support Vectors**: Points outside the Œµ-tube influence the fit.\n",
    "* **Kernel Trick**: Handles non-linear regression.\n",
    "* **Slack Variables**: Allows some points to be outside Œµ-tube.\n",
    "\n",
    "### üéØ Objective:\n",
    "\n",
    "* Fit a function with **maximum flatness** within Œµ-tube\n",
    "* Minimize the **error outside** the tube\n",
    "\n",
    "### üßæ Output:\n",
    "\n",
    "* Predicted **continuous values** (e.g., 10.5, 200.3, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Bonus: Common in Both\n",
    "\n",
    "| Concept                | What It Does                                        |\n",
    "| ---------------------- | --------------------------------------------------- |\n",
    "| **Kernel**             | Transforms data to make it linearly separable       |\n",
    "| **C (Regularization)** | Controls trade-off between margin size and error    |\n",
    "| **Gamma**              | Defines how far influence of a single point reaches |\n",
    "| **Support Vectors**    | Data points that \"hold up\" the decision boundary    |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary Table\n",
    "\n",
    "| Model   | Task           | Goal                                      | Output                             | Key Concept                 |\n",
    "| ------- | -------------- | ----------------------------------------- | ---------------------------------- | --------------------------- |\n",
    "| **SVC** | Classification | Maximize margin between classes           | Class label (e.g., `cat` or `dog`) | Support Vectors, Hyperplane |\n",
    "| **SVR** | Regression     | Fit within Œµ-tube with few outside points | Number (e.g., price, age)          | Epsilon Tube, Flat Line     |\n",
    "| **SVM** | General term   | Backbone for SVC & SVR                    | Either class or number             | Kernel, C, Gamma            |\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also draw a quick visual or give a real-world analogy (like a judge separating teams or a tightrope walker fitting a path). Want that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f1a6f-a500-444d-ba2b-07f9bbe132a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff36e87-1e33-49b7-a0fb-e990fc29c134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
